import json
import sys
import os

from pathlib import Path
from tqdm import tqdm

project_root = str(Path(__file__).parent.parent.absolute())
sys.path.append(project_root)

from src.transneft_ai_consultant.backend.rag.pipeline import rag_answer
from src.transneft_ai_consultant.backend.evaluation.metrics import calculate_bleurt, calculate_sas
from src.transneft_ai_consultant.backend.evaluation.rouge_ru import calculate_rouge_ru
from src.transneft_ai_consultant.backend.evaluation.metrics_ranking import (
    ndcg_mean_at_k, mrr_at_k, map_at_k
)
from src.transneft_ai_consultant.backend.rag.vector_store import query_documents
from src.transneft_ai_consultant.backend.evaluation.metrics_advanced import (
    calculate_bleurt_score,
    calculate_semantic_answer_similarity
)


BENCHMARK_FILE_PATH = os.path.join(project_root, "benchmark.json")
EVALUATION_RESULTS_FILE_PATH = os.path.join(project_root, "evaluation_results.json")


def evaluate_retriever(benchmark_data):


    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞."""
    qid_to_truth = {}
    qid_to_retrieved = {}

    for i, item in enumerate(tqdm(benchmark_data, desc="–û—Ü–µ–Ω–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞")):
        qid = f"q{i}"
        question = item["question"]
        true_context = item["context"]

        # –ü–æ–ª—É—á–∞–µ–º top-100 –¥–ª—è MAP@100
        retrieved = query_documents(question, top_k=100)
        retrieved_ids = [doc.get("metadata", {}).get("section_id", f"doc_{j}")
                         for j, doc in enumerate(retrieved)]

        # –ü–æ–∏—Å–∫ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ ID
        true_id = None
        for doc in retrieved:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Ç–µ–∫—Å—Ç—É
            if true_context.strip() in doc["context"] or doc["context"].strip() in true_context:
                true_id = doc.get("metadata", {}).get("section_id")
                break

        if true_id:
            qid_to_truth[qid] = {true_id}
            qid_to_retrieved[qid] = retrieved_ids

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
    if qid_to_truth and qid_to_retrieved:
        ndcg_10 = ndcg_mean_at_k(qid_to_truth, qid_to_retrieved, k=10)
        mrr_10 = mrr_at_k(qid_to_truth, qid_to_retrieved, k=10)
        map_100 = map_at_k(qid_to_truth, qid_to_retrieved, k=100)

        print("\n" + "=" * 50)
        print("--- –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ ---")
        print(f"NDCG@10:  {ndcg_10:.4f}")
        print(f"MRR@10:   {mrr_10:.4f}")
        print(f"MAP@100:  {map_100:.4f}")
        print("=" * 50 + "\n")

        return {
            "ndcg@10": ndcg_10,
            "mrr@10": mrr_10,
            "map@100": map_100
        }
    else:
        print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞")
        return {
            "ndcg@10": 0.0,
            "mrr@10": 0.0,
            "map@100": 0.0
        }


def main():
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    try:
        with open(BENCHMARK_FILE_PATH, 'r', encoding='utf-8') as f:
            benchmark_file = json.load(f)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞
        if isinstance(benchmark_file, dict) and "benchmark" in benchmark_file:
            benchmark = benchmark_file["benchmark"]
            print(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∞: {benchmark_file.get('metadata', {})}")
        else:
            benchmark = benchmark_file

    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª –±–µ–Ω—á–º–∞—Ä–∫–∞ {BENCHMARK_FILE_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/create_benchmark.py")
        return
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
        return

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(benchmark)} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞.\n")

    # –û—Ü–µ–Ω–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
    print("=" * 50)
    print("–≠–¢–ê–ü 1: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞")
    print("=" * 50)
    retriever_metrics = evaluate_retriever(benchmark)

    # –û—Ü–µ–Ω–∫–∞ QA —Å–∏—Å—Ç–µ–º—ã
    print("\n" + "=" * 50)
    print("–≠–¢–ê–ü 2: –û—Ü–µ–Ω–∫–∞ QA —Å–∏—Å—Ç–µ–º—ã")
    print("=" * 50)

    evaluation_results = []

    for item in tqdm(benchmark, desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"):
        question = item["question"]
        ground_truth_answer = item["ground_truth"]

        try:
            rag_result = rag_answer(question, use_reranking=True, use_ensemble=True, use_compression=True, log_demo=False)
            generated_answer = rag_result["answer"]
            retrieved_contexts = rag_result["retrieved_contexts"]
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {question[:50]}...")
            print(f"   –ü—Ä–∏—á–∏–Ω–∞: {e}")
            generated_answer = ""
            retrieved_contexts = []

        evaluation_results.append({
            "question": question,
            "ground_truth_answer": ground_truth_answer,
            "generated_answer": generated_answer,
            "retrieved_contexts": retrieved_contexts
        })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open(EVALUATION_RESULTS_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {EVALUATION_RESULTS_FILE_PATH}")

    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ QA
    print("\n" + "=" * 50)
    print("–≠–¢–ê–ü 3: –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞")
    print("=" * 50)

    ground_truths = [res["ground_truth_answer"] for res in evaluation_results]
    generated_answers = [res["generated_answer"] for res in evaluation_results]

    print("–†–∞—Å—á–µ—Ç BLEURT-20...")
    bleurt_score = calculate_bleurt(predictions=generated_answers, references=ground_truths)

    print("–†–∞—Å—á–µ—Ç ROUGE-L –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞...")
    rouge_scores = calculate_rouge_ru(predictions=generated_answers, references=ground_truths)
    rouge_l_score = rouge_scores["rougeL"].fmeasure

    print("–†–∞—Å—á–µ—Ç Semantic Answer Similarity...")
    sas_score = calculate_sas(predictions=generated_answers, references=ground_truths)

    # –ù–û–í–û–ï: –†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç—Ä–∏–∫
    print("\n–†–∞—Å—á–µ—Ç BLEURT-20 (BERTScore)...")
    bleurt_20_score = calculate_bleurt_score(predictions=generated_answers, references=ground_truths)

    print("–†–∞—Å—á–µ—Ç Semantic Answer Similarity (BGE-M3)...")
    sas_bge_m3 = calculate_semantic_answer_similarity(predictions=generated_answers, references=ground_truths)

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 70)
    print("üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò")
    print("=" * 70)
    print("\nüîç –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞:")
    print(f"   NDCG@10:  {retriever_metrics['ndcg@10']:.4f}")
    print(f"   MRR@10:   {retriever_metrics['mrr@10']:.4f}")
    print(f"   MAP@100:  {retriever_metrics['map@100']:.4f}")
    print("\nüí¨ –ú–µ—Ç—Ä–∏–∫–∏ QA —Å–∏—Å—Ç–µ–º—ã:")
    print(f"   BLEURT (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è): {bleurt_score:.4f}")
    print(f"   BLEURT-20 (BERTScore): {bleurt_20_score:.4f}")
    print(f"   ROUGE-L (F-score): {rouge_l_score:.4f}")
    print(f"   Semantic Similarity (—Å—Ç–∞—Ä–∞—è): {sas_score:.4f}")
    print(f"   Semantic Answer Similarity (BGE-M3): {sas_bge_m3['average']:.4f}")
    print("=" * 70 + "\n")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final_metrics = {
        "retriever": retriever_metrics,
        "qa_system": {
            "bleurt_old": bleurt_score,
            "bleurt_20": bleurt_20_score,
            "rouge_l_fscore": rouge_l_score,
            "semantic_similarity_old": sas_score,
            "semantic_answer_similarity_bge_m3": sas_bge_m3['average']
        },
        "benchmark_size": len(benchmark),
        "timestamp": datetime.now().isoformat()
    }

    metrics_file = os.path.join(project_root, "final_metrics.json")
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(final_metrics, f, ensure_ascii=False, indent=2)

    print(f"üìÅ –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metrics_file}")

    def evaluate_hallucination_resistance(benchmark_data):
        '''–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≥–æ–≤–æ—Ä–∏—Ç –ª–∏ —Å–∏—Å—Ç–µ–º–∞ "–Ω–µ –∑–Ω–∞—é" –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–∞.'''

        negative_samples = [item for item in benchmark_data if item.get('type') == 'unanswerable']

        correct_rejections = 0
        for item in negative_samples:
            answer = rag_answer(item['question'])

            refusal_phrases = [
                '–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
                '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö',
                '–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å',
                '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º –Ω–µ—Ç'
            ]

            if any(phrase in answer.lower() for phrase in refusal_phrases):
                correct_rejections += 1

        hallucination_resistance = correct_rejections / len(negative_samples) if negative_samples else 0
        return hallucination_resistance



if __name__ == "__main__":
    from datetime import datetime

    main()
