import json
import sys
from pathlib import Path
from tqdm import tqdm

project_root = str(Path(__file__).parent.parent.absolute())
sys.path.insert(0, project_root)

from src.transneft_ai_consultant.backend.rag.pipeline import answer_question
from src.transneft_ai_consultant.backend.evaluation.metrics import initialize_metrics, calculate_all_metrics
from src.transneft_ai_consultant.backend.evaluation.metrics_ranking import (
    ndcg_mean_at_k,
    mrr_at_k,
    map_at_k
)
from src.transneft_ai_consultant.backend.config import ROOT_DIR

BENCHMARK_PATH = ROOT_DIR / "benchmarks" / "benchmark.json"
RESULTS_PATH = ROOT_DIR / "benchmarks" / "evaluation_results.json"
FINAL_METRICS_PATH = ROOT_DIR / "benchmarks" / "final_metrics.json"


def main():
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫...")
    initialize_metrics()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞
    with open(BENCHMARK_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, dict) and "questions" in data:
        questions = data["questions"][:10]
    elif isinstance(data, list):
        questions = data[:10]
    else:
        raise ValueError("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç benchmark.json!")

    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞.\n")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    print("\n" + "=" * 50)
    print("–≠–¢–ê–ü 2: –û—Ü–µ–Ω–∫–∞ QA —Å–∏—Å—Ç–µ–º—ã")
    print("=" * 50)

    results = []
    for item in tqdm(questions, desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"):
        q = item["question"]
        answer, context_docs = answer_question(q)

        results.append({
            "question_id": item.get("question_id", f"q{len(results) + 1}"),
            "question": q,
            "reference_answer": item.get("ground_truth_answer", item.get("answer", "")),
            "generated_answer": answer,
            "context_docs": context_docs,
            "relevant_docs": item.get("relevant_docs", [])  # ‚Üê –°–û–•–†–ê–ù–Ø–ï–ú relevant_docs!
        })

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    with open(RESULTS_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {RESULTS_PATH}")

    # –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    print("\n" + "=" * 50)
    print("–≠–¢–ê–ü 3: –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞")
    print("=" * 50)

    valid_pairs = [
        (r["reference_answer"], r["generated_answer"])
        for r in results
        if r["reference_answer"] and r["reference_answer"].strip()
    ]

    if valid_pairs:
        references, predictions = zip(*valid_pairs)
        gen_metrics = calculate_all_metrics(list(references), list(predictions))
        print(f"‚úÖ –†–∞—Å—á–∏—Ç–∞–Ω–æ –º–µ—Ç—Ä–∏–∫ QA –¥–ª—è {len(valid_pairs)} –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —ç—Ç–∞–ª–æ–Ω–∞–º–∏")
    else:
        gen_metrics = {"bleurt": 0.0, "rouge_l": 0.0, "sas": 0.0}
        print("‚ö†Ô∏è –ù–µ—Ç —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ benchmark.json, –º–µ—Ç—Ä–∏–∫–∏ QA = 0")

    # –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\n–†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è...")

    qid_to_truth = {}
    qid_to_retrieved = {}
    use_real_ground_truth = False

    for result in results:
        qid = result["question_id"]

        # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú –†–ï–ê–õ–¨–ù–´–ï relevant_docs –ò–ó benchmark.json
        if result.get("relevant_docs") and len(result["relevant_docs"]) > 0:
            qid_to_truth[qid] = set(result["relevant_docs"])
            use_real_ground_truth = True
        else:
            # –§–æ–ª–ª–±—ç–∫: —Ç–æ–ø-3 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            qid_to_truth[qid] = set([
                doc["metadata"].get("section_id", f"doc_{i}")
                for i, doc in enumerate(result["context_docs"][:3])
            ])

        # Retrieved: –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        qid_to_retrieved[qid] = [
            doc["metadata"].get("section_id", f"doc_{i}")
            for i, doc in enumerate(result["context_docs"])
        ]

    ndcg_5 = ndcg_mean_at_k(qid_to_truth, qid_to_retrieved, k=5)
    mrr_10 = mrr_at_k(qid_to_truth, qid_to_retrieved, k=10)
    map_100 = map_at_k(qid_to_truth, qid_to_retrieved, k=100)

    ranking_metrics = {
        "ndcg@5": round(ndcg_5, 4),
        "mrr@10": round(mrr_10, 4),
        "map@100": round(map_100, 4)
    }

    if use_real_ground_truth:
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ relevant_docs!")
    else:
        print(f"‚ö†Ô∏è –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Å–µ–≤–¥–æ-—ç—Ç–∞–ª–æ–Ω (—Ç–æ–ø-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞)")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
    final_metrics = {**gen_metrics, **ranking_metrics}

    with open(FINAL_METRICS_PATH, "w", encoding="utf-8") as f:
        json.dump(final_metrics, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 70)
    print("üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò")
    print("=" * 70)
    print("\nüí¨ –ú–µ—Ç—Ä–∏–∫–∏ QA —Å–∏—Å—Ç–µ–º—ã:")
    print(f"   BLEURT: {gen_metrics['bleurt']:.4f}")
    print(f"   ROUGE-L: {gen_metrics['rouge_l']:.4f}")
    print(f"   SAS: {gen_metrics['sas']:.4f}")
    print("\nüîç –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"   NDCG@5: {ranking_metrics['ndcg@5']:.4f}")
    print(f"   MRR@10: {ranking_metrics['mrr@10']:.4f}")
    print(f"   MAP@100: {ranking_metrics['map@100']:.4f}")
    print("=" * 70)
    print(f"\nüìÅ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {FINAL_METRICS_PATH}")


if __name__ == "__main__":
    main()

